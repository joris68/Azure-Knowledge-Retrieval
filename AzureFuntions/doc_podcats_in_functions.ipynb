{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vektordatenbank im Podcastexplorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Notebook soll einen guten Überblick darübr geben, wie man mit Vektordatenbanken in Zusammenhang mit Large-Language-Models (LLMs) arbeitet. \n",
    "Under anderem inkludiert das folgende Teilaufgaben und Konzepte: Textchunking, Vectorembedding, Metadaten, Indizess, Similarity Search.\n",
    "\n",
    "Die Implementierung erfoglt in Pinecone (mit dem Python Pinecone-client) , der openAI library, und dem Langchain Framework und den schon transkriebierten Podcastfolgen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als erstes möchte ich eine Definitionen vorweg nehmen.\n",
    "\n",
    "Tokenisierung: Der Prozess der Aufteilung einer Zeichenkette (in unserem Fall ein transkribierte Podacastfolge) in Tokens, wird als Tokenisierung bezeichnet. Tokens entsprechen in englischer Sprache durchschnittlich vier Zeichen.\n",
    "\n",
    "Tokenisierung wird von einem Tokenizer übernommen in python mit der tiktoken library. Ziel des ganzen ist das Textdokument in vielzahl von \"Chunks\" aufzuteilen, die aus einer vorgegebenen Anzahl aus Tokens bestehen, aber alles noch im Klartext.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ich entscheide mich diesmal gegen den Blob-Storage Connector aus dem Langchain Framework aus dem einfachen Grund, weil ich diesmal den Blobnamen als Metadatum zum jedem Vector mitabspeichern möchte, das macht das arbeiten mit dem Blobtrigger um einiges einfacher. Um Files hereinzuladen, und auch wieder zu löschen...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "als erstes definieren wir uns die lenght funktionen, die vom Textsoplitter genutzt wird, den trankribierten text in Chunks gewünschter länge zu verteilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alle benötigten \n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import pinecone\n",
    "from common import upload_blob_to_storage\n",
    "import azure.functions as func\n",
    "from support_allInOne import  search_pinecone, generate_JSON_respone, ask_GTP_api, generate_container_sas, generate_prompt_input, check_for_answer_capacity_in_tokens\n",
    "import traceback\n",
    "import openai\n",
    "from azure.storage.blob import BlobServiceClient, generate_container_sas, BlobSasPermissions\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\jgabrisch\\AppData\\Local\\Microsoft\\WindowsApps\\python3.10.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/jgabrisch/AppData/Local/Microsoft/WindowsApps/python3.10.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "#als erstes definieren wir uns die lenght funktionen, die vom Textsoplitter genutzt wird, den trankribierten text in Chunks gewünschter länge zu verteilen\n",
    "# als erstes schreiben wir uns die \"Lenght Function\"  die das Von uns angegebene tokenizer Model benutzt\n",
    "# diese Lenght Function übergeben wir dem Textsplitter objekt der für uns das splitten übernimmt\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base') # das tokenizer model\n",
    "\n",
    "\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "\n",
    "    try:\n",
    "\n",
    "        tokens = tokenizer.encode(\n",
    "            text,\n",
    "            disallowed_special=()\n",
    "        )\n",
    "        return len(tokens)\n",
    "    \n",
    "    except:\n",
    "        logging.error(\"Something went wrong in the tiktoken_len\")\n",
    "        #raise Exception(\"Something went wrong in the tiktoken_len\")\n",
    "\n",
    "\n",
    "# initialisieren des Textsplitters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "chunk_size=400, # 400 hat sich als gute chunk size herausgestellt\n",
    "chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "length_function=tiktoken_len, \n",
    "separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. wir connecten uns mit der python SDK zu der Blob-Storage in der die Files liegen und lassen uns eine Blob liste generieren\n",
    "# 2. Wir generieren uns eine Blob-List und können so auf einzelne Blob und deren Metadaten (Properties zugreifen)\n",
    "\n",
    "\n",
    "# Im Quell Ordner gibt es eine Unterordner für 2021 / 2022 / 2023 damit die Funktion nicht zu lange läuft, lasse ich die Funktion dreimal laufen (manuell, weil den relativen Blob path händisch umstelle)\n",
    "# das machen wir für jedes Jahr\n",
    "\n",
    "\n",
    "connection_string = os.environ['storage_PodcastExplorer']\n",
    "        \n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "      \n",
    "container_client = blob_service_client.get_container_client('transcriptions')\n",
    "\n",
    "blob_list = container_client.list_blobs(name_starts_with= 'Angebissen - der Angelpodcast/2021')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir definieren und die Sink connection\n",
    "\n",
    "app_setting = 'my_storage'\n",
    "\n",
    "connection_string = os.environ[app_setting] \n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "container_client = blob_service_client.get_container_client('cleartextchunks/2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Wir unterteilen jeden einzelne Dokument in Klartext chunk und speichern den dazugehörige  und Blobnamen und de reltiven Source Path an (für spätere SAS-Token generierung) ab in einem Python Dictionary\n",
    "# 4. Wir erstellen für jeden Dokument ein Json Dokument (python dictionary wird zum json geparsed) und speichern es in einer Blob Storage ab (als JSON-File)\n",
    "\n",
    "start_point = 'https://stpodcastexplorer.blob.core.windows.net/'\n",
    "relative_path = 'transcriptions/'\n",
    "\n",
    "for blob in blob_list:\n",
    "\n",
    "            text_chunks_with_metaData = []\n",
    "\n",
    "            # connect to the specific blob\n",
    "            blob_client = container_client.get_blob_client(blob)\n",
    "            \n",
    "            data = blob_client.download_blob()\n",
    "\n",
    "            # chunking the text of the blob into several chunks of size 400\n",
    "            chunks = text_splitter.split_text(data.content_as_text())\n",
    "\n",
    "            for txt in chunks:\n",
    "\n",
    "                a = {\n",
    "        \n",
    "                    'text' : txt, \n",
    "                    'blob_url': start_point + relative_path + str(data.name),\n",
    "                    'blob_name': data.name\n",
    "\n",
    "                }\n",
    "\n",
    "                text_chunks_with_metaData.append(a)\n",
    "\n",
    "            try:\n",
    "                # parse python dictinary to json data\n",
    "                json_data = json.dumps(text_chunks_with_metaData)\n",
    "                # we do  not want the '.txt' ending\n",
    "                output_blob = container_client.get_blob_client(str(data.name[:-4]) + '_chunked.json')\n",
    "                output_blob.upload_blob(json_data,overwrite=True)\n",
    "                logging.info(\"Blob for file : \" + data.name + ' was successfully written to storage' )\n",
    "\n",
    "            except:\n",
    "\n",
    "                logging.warning(\"Blob for file : \" + data.name + ' could NOT BE WRITTEN!!!!' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir jetzt den ersten schritt gemacht haben und die Klartext chunks erstellt haben. Werden wir diese jetzt in Vektoren übersetzen (embedden) lassen via der OpenAI api und dem Text-embedding model:\n",
    "\n",
    "Vektorembedding: Voktorembedding von Klartext-Chunks bezieht sich auf den Prozess, bei dem Wörter oder Textabschnitte als numerische Vektoren repräsentiert werden, um deren semantische Bedeutung und kontextuelle Beziehungen zu erfassen. Diese Technik wird häufig in der Verarbeitung natürlicher Sprache (NLP) und maschinellem Lernen eingesetzt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Index in einer Vektordatenbank beschreibt meistens nur eine \"Datenbank\". Innerhalb dieses Indexes (Datenbank) kannst du mehrere Collections, Namspaces o.Ä haben (Wie mehrere Tabellen in einer Datenbank). Da kommt es ganz auf den Anbieter drauf an wie er das genau benennt.\n",
    "\n",
    "Der Begriff Index hat sich durchgesetzt für nichts weiter als einen Speicherort, weil du durch den hohen Berechnungsaufwand der \"Similarity Search\" im Vektorraum Mathematische Heuristiken entwickelt werden mussten, um den Exponentiellen Berechnungsaufwand logarithmisch skalieren zu lassen. Ein sogenannter Index tut genau dieses. (Mit FAISS der openSource library von Facebook kann man das selbst programmatisch ausprobieren)\n",
    "\n",
    "Aber da dieser \"Index\" (der als Datenbank fungiert) einer Vektordatnebank unsere Vektoren automatisch Indiziert, wird er meistend Index genannt.\n",
    "\n",
    "Similarity Search: Die Similarity Searchbezieht sich auf die Suche nach ähnlichen Elementen oder Datensätzen in einer Vektordatenbank. Dafür gibt es verschiedene Mathematische Ansätze die Ähnlichkeit von Vektoren zu bestimmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warum sind Metadaten wichtig:\n",
    "Metadaten sind wichtig im Zusammenhang mit Vektordatenbanken, da sie zusätzliche Informationen über die gespeicherten Vektoren liefern. Diese zusätzlichen Informationen ermöglichen eine bessere Verwaltung, Suche und Analyse der Daten in der Datenbank. Für den Menschen sind Vektoren nicht Human-Readable. Metadaten geben den Vektoren Bedeutung und Kontext. Ohne Metadaten sind Vektoren nur numerische Werte ohne klare Interpretation. \n",
    "Metadaten ermöglichen eine präzisere Suche und Filterung in der Datenbank, da man den Search Space verkleinern kann. Sie können Vektoren in logische oder semantische Gruppen einteilen oder zeitliche Abhängigkeiten hinzufügen. Und noch vieles weitere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wir nehmen uns die davor erstellten Json-Dokumente \n",
    "#2. Wir embedden den Klartext zu einem Vektor via der OpenAI API, dort werden wir das \"text-embedding-ada-002\" model benutzen\n",
    "#3. Mit Regex ziehen wir uns die Metadaten zu den Vektoren aus dem Blob name heraus und speichern sie im JSON format ab (BEACHTE: die meisten Voktordatenbanken folgen dem NOSQL prinzip, das Json-Schema ist nicht fest vorgegeben und man kann jedem Vektor eine beliebieg Menge an Metadaten mitgeben)\n",
    "#4. Über den Pinecone client inserten wir (upserten) den Vektor inklusive metadaten und klartext in unseren vorher definierten Index.\n",
    "\n",
    "\n",
    "connection_string = os.environ['my_storage']\n",
    "       \n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "      \n",
    "container_client = blob_service_client.get_container_client('cleartextchunks')\n",
    "\n",
    "# hier laden wir uns wieder die Klartext chunks in eine BlobListe\n",
    "blob_list = container_client.list_blobs(name_starts_with= 'Angebissen - der Angelpodcast/2023')\n",
    "\n",
    "logging.info(\"got the Blob List\")\n",
    "      \n",
    "# hier verbinden wir uns zu unserem Vektordatnebank auf Pinecone\n",
    "pinecone.init(      \n",
    "api_key='<<api-key>>',      \n",
    "environment='asia-southeast1-gcp-free'      \n",
    ")      \n",
    "index = pinecone.Index('mypodcastindex')\n",
    " \n",
    "      \n",
    "logging.info(\"start embedding\")\n",
    "embeddings = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\")\n",
    "\n",
    "# list slicing for testing purposes\n",
    "\n",
    "for blob in blob_list:\n",
    "\n",
    "  vectors = []\n",
    "\n",
    "  blob_client = container_client.get_blob_client(blob)\n",
    "            \n",
    "  data = blob_client.download_blob()\n",
    "\n",
    "  json_data = json.loads(data.content_as_text())\n",
    "\n",
    "        # update identity\n",
    "  stats_response = index.describe_index_stats()\n",
    "\n",
    "  # calculate identity counter\n",
    "  identity_counter = stats_response['total_vector_count'] +1\n",
    "  chunk_counter = 1\n",
    "\n",
    "  for chunk in json_data:\n",
    "\n",
    "      # hier ziehen wir uns die Metadaten die in den BlobNamen gespeichert sind raus\n",
    "\n",
    "      # für die Episodennummer\n",
    "      match_sequence = re.search(r\"/(\\d+)_\", chunk['blob_name_short'])\n",
    "      if match_sequence:\n",
    "            episode = match_sequence.group(1)\n",
    "      else:\n",
    "            episode = \"\"\n",
    "\n",
    "      # für das Erscheinungsdaten\n",
    "      match_date = re.search(r\"(\\d{4})(\\d{2})(\\d{2})\", chunk['blob_name_short'])\n",
    "      if match_date:\n",
    "            date = f\"{match_date.group(1)}{match_date.group(2)}{match_date.group(3)}\"\n",
    "      else:\n",
    "            date = \"\"\n",
    "\n",
    "      # für den Titel\n",
    "      match_title = re.search(r\"/(\\d+_\\d+_)(.+?)_\", chunk['blob_name_short'])\n",
    "      if match_title:\n",
    "            title = match_title.group(2).replace(\"_\", \" \")\n",
    "      else:\n",
    "            title = \"\"\n",
    "\n",
    "       # so definieren wir unser upsert Objekt, wie und was man alles abspeichern kann hängt von dem Anbieter     \n",
    "      curr = {\n",
    "                  \"id\": \"vec\" + str(identity_counter),\n",
    "                  \"values\" : embeddings.embed_query(chunk['text']),\n",
    "                  \"metadata\": {\n",
    "                  \"blob_url\": chunk['blob_url'], \n",
    "                  \"year\" : '2023',\n",
    "                  \"date\" : date, \n",
    "                  \"episode\" : episode,\n",
    "                  \"title\": title,\n",
    "                  \"clear_text\": chunk['text'],\n",
    "                  \"blob_name\": chunk['blob_name_short'],\n",
    "                  \"chunk_counter\": chunk_counter,\n",
    "                  \"total_doc_chunks\" : len(json_data)\n",
    "                  }        \n",
    "      }\n",
    "\n",
    "      identity_counter += 1\n",
    "      chunk_counter +=1 \n",
    "\n",
    "\n",
    "      vectors.append(curr)\n",
    "\n",
    "      # insert Vector + Metadata into DB\n",
    "      index.upsert(vectors=[curr], namespace=\"angebissen\")\n",
    "\n",
    "      #--------------- innere For Schleife beendet--------------------------\n",
    "\n",
    "# hier definieren wir uns den JSON-File den wir uns abspichern für den Db Inhalt\n",
    "  json_to_return = {\n",
    "      \"vectors\": vectors,\n",
    "\n",
    "      \"namespace\": \"angebissen\"\n",
    "\n",
    "  }\n",
    "         \n",
    "  upload_blob_to_storage('embeddedchunks', json.dumps(json_to_return), data.name[:-5] +'_upserted.json')\n",
    "\n",
    "#--------------------- äußere For schleife beendet------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir alles in die Datenbank inserted haben, wollen wir die Datenbank auch benutzen! Zeit Fragen zu stellen und das läuft so ab.\n",
    "\n",
    "1. Wir bekommen eine Frage gestellt vom User (unsere Function wird via HTTP Trigger getriggert per Post Request)\n",
    "2. Aus dem JSON Body extrahieren wir uns die Frage und embedden diese. Nun haben wir einen Search Vector (!!! Wichig: Wir müssen das gleiche Embedding Model benutzen, sonst vergleichen wir Äpfel mit Birnen!!!)\n",
    "3. Jeztt performen wir die \"Similarity Search\". Die Vektordatenbank gibt uns die k most similar Vektoreinträge zurück inklusive Metadaten, denn diese interessieren uns eigentlich. (k ist ein natürlicher Parameter, der nur durch die Chunk Size und Tokenin- und Tokenoutput vom LLM bedingt wird)\n",
    "4. Wir setzen uns einen prompt zusammen. Das Prompt Template ist in einem Textfile ausgelagert. Dort schreiben wir den Kontext rein (extrahierte Klartext zu den Vektoren und Metadaten, die wir zu einem großen String konkatenieren) und eben die User Frage\n",
    "5. Wir schicken diesen Prompt zu ChatGTP und erhalten eine genaue Antwort auf Basis unsere Daten!\n",
    "6. Wie erstellen eine JSON Antwort, die die Frage, die Antwort, die DB Response und SAS-Links zu den referenzioerten Dokumenten in der Datenbank.\n",
    "7. Wir schicken diese zurück und das Frontend muss den Rest übernehmen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "         \n",
    "        # get the Question out of the Request Body\n",
    "\n",
    "        req_body = req.get_json()\n",
    "\n",
    "\n",
    "        question = req_body['question']\n",
    "\n",
    "        # embed the query to a vector\n",
    "        embeddings = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\")\n",
    "        question_embedding = embeddings.embed_query(question)\n",
    "\n",
    "\n",
    "        # ------------------ now query the database\n",
    "        # top_k als parameter für die Anzahl für die most similar einträge die zurück gegeben werden\n",
    "        query_response = search_pinecone(question_embedding, top_k=6)\n",
    "\n",
    "        # ---------------- now generate the answer --------------------\n",
    "\n",
    "        # hier wird die große Json zurückgegeben \n",
    "        response = generate_JSON_respone(ask_GTP_api(query_response, question), query_response, question)\n",
    "        \n",
    "\n",
    "        logging.info(\"Generated answer , sending it back\")\n",
    "\n",
    "        #return func.HttpResponse(json.dumps(response), status_code=200)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "        error_traceback = traceback.format_exc()\n",
    "\n",
    "        #return func.HttpResponse(\"Irgendwas ist schiefgelaufen:\" + str(e) + \" --------\" + error_traceback , status_code=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen, die in einen Python File ausgelagert habe, um den Code \"sauber\" zu Halten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier setzen wir uns den sogenannten Context für den Prompt zusammen. Wir benutzen die Metadaten Epsiodennummer, Titel und berechnen wo im Dokuemnt diese Referenz liegt\n",
    "\n",
    "def generate_content_for_prompt(query_response):\n",
    "    try:\n",
    "\n",
    "        logging.info(\"now we are concatenating the input\")\n",
    "\n",
    "\n",
    "        string_to_return = \"\"\n",
    "        for i in query_response['matches']:\n",
    "\n",
    "            # hier berechnen wir uns die Stelle , wo es im Dokument vorkommt\n",
    "            a = i['metadata']['chunk_counter'] /i['metadata']['total_doc_chunks']\n",
    "\n",
    "            if a >= 0.75:\n",
    "                meta_zeit = 'Am Ende'\n",
    "            elif a < 0.75 and a > 0.25:\n",
    "                meta_zeit = \"In der Mitte\"\n",
    "            else:\n",
    "                meta_zeit = 'Am Anfang'\n",
    "\n",
    "            string_to_return += str(i['metadata']['clear_text']) + meta_zeit +\" der Folge \" + str(i['metadata']['episode']) + \" mit dem Name \" + str(i['metadata']['title']) + \".\"\n",
    "\n",
    "\n",
    "        return string_to_return\n",
    "    \n",
    "    except:\n",
    "        logging.error(\"Something went wrong genrating the Content for the Prompt\")\n",
    "        raise Exception(\"Something went wrong genrating the Content for the Prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lese ich das Prompt Template aus dem Textfile heraus und setze meine \"Bausteine\" ein\n",
    "\n",
    "def generate_prompt_input(context, question):\n",
    "    with open('prompt_template.txt') as f:\n",
    "        prompt = f.read()\n",
    "    \n",
    "    prompt = prompt.replace(\"<<Context>>\", context)\n",
    "    prompt = prompt.replace(\"<<Frage>>\", question)\n",
    "\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so gebe ich eine query in Pinecone ab, top_k =5 ist als default wert eingestellt.\n",
    "\n",
    "def search_pinecone(question_embedding, top_k = 5):\n",
    "\n",
    "    pinecone.init(      \n",
    "\t    api_key='<<api-key>>',      \n",
    "\t    environment='asia-southeast1-gcp-free'      \n",
    "        )      \n",
    "    index = pinecone.Index('mypodcastindex')      \n",
    "\n",
    "    query_response = index.query(\n",
    "            top_k=top_k,\n",
    "            include_values=False, # wir brauchen die Vektoren Values nicht\n",
    "            include_metadata=True, # wire brauchen auf jeden fall die metadaten\n",
    "            vector=question_embedding,\n",
    "            namespace='angebissen'\n",
    "        )\n",
    "    \n",
    "    return query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jetzt haben wir alles zusammen, um einen prompt an \"chatGTP\" zu stellen das mache wir über die openai library\n",
    "\n",
    "def ask_GTP_api(query_response, question):\n",
    "\n",
    "    context = generate_content_for_prompt(query_response)\n",
    "\n",
    "\n",
    "    prompt = generate_prompt_input(context, question)\n",
    "\n",
    "\n",
    "    openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "    openai.api_base= os.environ['OPENAI_API_BASE']\n",
    "    openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "    openai.api_version =os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "    # diese funktion monitored unseren input und berechnet den maximal mnöglichen output für das LLM\n",
    "    max_tokens = check_for_answer_capacity_in_tokens(prompt)\n",
    "\n",
    "    # wir wollen die temparture bei 0 halten\n",
    "    completion = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, temperature=0, max_tokens=max_tokens)\n",
    "\n",
    "    return completion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so generiere SAS Tokens, sodass der User direkt auf die Dokumente zugreifen kann auch wenn er nicht Contributer der Blob storage ist oder ähnliches\n",
    "\n",
    "def generate_SAS_urls_for_sources(query_response):\n",
    "\n",
    "    try:\n",
    "\n",
    "\n",
    "        # now we are going to \n",
    "\n",
    "        start_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "        expiry_time = start_time + datetime.timedelta(days=1)\n",
    "\n",
    "        app_setting = 'storage_PodcastExplorer'\n",
    "\n",
    "        connection_string = os.environ[app_setting] \n",
    "\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "        container_client = blob_service_client.get_container_client('transcriptions')\n",
    "\n",
    "        sas_token = generate_container_sas(\n",
    "            account_name=container_client.account_name,\n",
    "            container_name=container_client.container_name,\n",
    "            permission=BlobSasPermissions(read=True),\n",
    "            account_key=container_client.credential.account_key, \n",
    "            expiry=expiry_time,\n",
    "            start=start_time\n",
    "        )\n",
    "        \n",
    "\n",
    "        # now concatenate\n",
    "        finished_links = []\n",
    "\n",
    "        for x in query_response['matches']:\n",
    "\n",
    "            a =  x['metadata']['blob_url'] + '?' + str(sas_token)\n",
    "\n",
    "            finished_links.append(a)\n",
    "\n",
    "        return finished_links\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        logging.error('SAS token genration hat nicht so gut funktioniert')\n",
    "        raise Exception('SAS token genration hat nicht so gut funktioniert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# am ende setzten wir einfach alles zusammen und schicken die json antwort an frontend zurück\n",
    "\n",
    "def generate_JSON_respone(completion, query_response, question):\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "    \n",
    "        \"GTP_answer\": completion.choices[0].text,\n",
    "\n",
    "        \"query_response\": str(query_response['matches']),\n",
    "\n",
    "        \"SAS_links\": generate_SAS_urls_for_sources(query_response)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sieht mein Prompt aus:\n",
    "\n",
    "Nutze ausschließlich den folgenden Kontext, um eine konkrete Antwort auf die Frage am Ende zu geben. Wenn du die Frage nicht beantworten kannst, antworte mit \"Das kann ich nicht beantworten\".\n",
    "Referenziere in deiner Antwort aus welcher Folge und Episoden, du die Informationen herausnimmst ebenfalls wann es in der Folge vorkommt. Halte dich dabei aber kurz! Schreibe einen fließenden Text.\n",
    "###\n",
    "<<Context>>\n",
    "###\n",
    "\n",
    "Frage: <<Frage>>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
